{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d45936",
   "metadata": {},
   "source": [
    "# START / CONFIGURE SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3471921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any existing Spark session (useful in notebooks)\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Old Spark session stopped.\")\n",
    "except Exception:\n",
    "    print(\"No active Spark session — starting fresh.\")\n",
    "\n",
    "# Basic local Spark config (adjust cores/memory as needed)\n",
    "SPARK_CORES   = 12          # Number of local cores to use\n",
    "DRIVER_MEM    = \"64g\"       # Driver memory\n",
    "SHUFFLE_PARTS = 320         # Global shuffle partitions\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"dk-housing-sqm-price\")\n",
    "        .master(f\"local[{SPARK_CORES}]\")\n",
    "        .config(\"spark.driver.memory\", DRIVER_MEM)\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(SHUFFLE_PARTS))\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad75257",
   "metadata": {},
   "source": [
    "# LOAD PARQUET + FIX DATE/TIME COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb479e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Fix legacy Parquet timestamp behavior (needed for some older files)\n",
    "spark.conf.set(\"spark.sql.legacy.parquet.nanosAsLong\", \"true\")\n",
    "spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseMode\", \"CORRECTED\")\n",
    "\n",
    "# Load main cleaned dataset\n",
    "df = spark.read.parquet(\"DKHousingPrices.parquet\")\n",
    "\n",
    "# Convert nanosecond epoch \"date\" → proper date, and extract time features\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\n",
    "        \"date\",\n",
    "        F.to_date(\n",
    "            F.from_unixtime(F.col(\"date\") / 1_000_000_000)  # ns → s → timestamp → date\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"year\",   F.year(\"date\"))\n",
    "    .withColumn(\"month\",  F.month(\"date\"))\n",
    "    # optional, not used later but nice to have:\n",
    "    .withColumn(\"quarter\", F.quarter(\"date\"))\n",
    ")\n",
    "\n",
    "# Repartition for smoother downstream operations and cache in memory\n",
    "SHUFFLE_PARTS = 200\n",
    "df = df.repartition(SHUFFLE_PARTS).cache()\n",
    "\n",
    "print(\"Materializing cache...\")\n",
    "row_count = df.count()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"SUCCESS! {row_count:,} Danish house sales loaded and cached\")\n",
    "print(\"=\" * 80)\n",
    "df.printSchema()\n",
    "\n",
    "# Quick preview: newest and oldest sales\n",
    "print(\"\\nMost recent sales:\")\n",
    "df.select(\"date\", \"year\", \"quarter\", \"house_type\", \"purchase_price\", \"sqm\") \\\n",
    "  .orderBy(F.desc(\"date\")) \\\n",
    "  .show(5, truncate=False)\n",
    "\n",
    "print(\"\\nOldest sales:\")\n",
    "df.select(\"date\", \"year\", \"quarter\", \"house_type\", \"purchase_price\", \"sqm\") \\\n",
    "  .orderBy(F.asc(\"date\")) \\\n",
    "  .show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fb840f",
   "metadata": {},
   "source": [
    "# DEFINE FEATURES + MISSING-VALUE CLEANING / IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8afe637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Feature lists (aligned with your dataset)\n",
    "cat_nominal = [\"house_type\", \"sales_type\", \"area\", \"region\", \"city\"]\n",
    "cat_zip     = [\"zip_code\"]\n",
    "\n",
    "num_features = [\n",
    "    \"sqm\",\n",
    "    \"%_change_between_offer_and_purchase\",\n",
    "    \"nom_interest_rate%\",\n",
    "    \"dk_ann_infl_rate%\",\n",
    "    \"yield_on_mortgage_credit_bonds%\",\n",
    "    \"year_build\",\n",
    "    \"no_rooms\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "]\n",
    "\n",
    "feature_columns = cat_nominal + cat_zip + num_features\n",
    "target = \"sqm_price\"\n",
    "\n",
    "print(f\"Total rows before any cleaning: {df.count():,}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Missing-value analysis\n",
    "# ------------------------------------------------------------------\n",
    "start_time = time.time()\n",
    "total_rows = df.count()\n",
    "\n",
    "missing_summary = []\n",
    "for col_name, col_type in df.dtypes:\n",
    "    # Only inspect feature + target columns\n",
    "    if col_name not in feature_columns + [target]:\n",
    "        continue\n",
    "\n",
    "    if col_type in (\"float\", \"double\", \"int\", \"bigint\", \"smallint\", \"tinyint\"):\n",
    "        miss = df.filter(\n",
    "            F.col(col_name).isNull() | F.isnan(F.col(col_name))\n",
    "        ).count()\n",
    "    else:\n",
    "        miss = df.filter(F.col(col_name).isNull()).count()\n",
    "\n",
    "    missing_summary.append((col_name, miss, round(miss / total_rows * 100, 4)))\n",
    "\n",
    "miss_df = pd.DataFrame(missing_summary, columns=[\"Column\", \"Missing\", \"%\"])\n",
    "miss_df = miss_df.sort_values(\"%\", ascending=False)\n",
    "\n",
    "print(\"\\nMISSING VALUES BEFORE CLEANING\")\n",
    "print(miss_df.to_string(index=False))\n",
    "\n",
    "# No columns >95% missing → keep all\n",
    "cols_to_drop = []\n",
    "feature_columns = [c for c in feature_columns if c not in cols_to_drop]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Drop rows where ALL features are null (rare, but safe)\n",
    "# ------------------------------------------------------------------\n",
    "df = df.dropna(how=\"all\", subset=feature_columns)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Drop rows with missing/NaN target (sqm_price)\n",
    "# ------------------------------------------------------------------\n",
    "df = df.filter(\n",
    "    F.col(target).isNotNull() & ~F.isnan(F.col(target))\n",
    ")\n",
    "print(f\"\\nRows after removing missing target: {df.count():,}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Build numeric vs categorical lists based on actual dtypes\n",
    "# ------------------------------------------------------------------\n",
    "numeric_cols = [\n",
    "    c for c, t in df.dtypes\n",
    "    if c in feature_columns\n",
    "    and t in (\"int\", \"bigint\", \"float\", \"double\", \"smallint\", \"tinyint\")\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    c for c in feature_columns\n",
    "    if c not in numeric_cols\n",
    "]\n",
    "\n",
    "print(f\"\\nNumeric columns to impute   : {len(numeric_cols)} → {numeric_cols}\")\n",
    "print(f\"Categorical columns (fill 'missing') : {len(categorical_cols)} → {categorical_cols}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Compute means for numeric columns\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\nComputing means for numeric columns...\")\n",
    "mean_exprs = [F.mean(c).alias(f\"mean_{c}\") for c in numeric_cols]\n",
    "means_row  = df.select(*mean_exprs).first()\n",
    "mean_dict  = {c: means_row[f\"mean_{c}\"] for c in numeric_cols}\n",
    "broadcast_means = spark.sparkContext.broadcast(mean_dict)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6) Apply imputation: numeric → mean, categorical → \"missing\"\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Imputing missing values...\")\n",
    "df = (\n",
    "    df.select(\n",
    "        *[\n",
    "            # Numeric → fill with pre-computed mean\n",
    "            F.coalesce(F.col(c), F.lit(broadcast_means.value.get(c, 0.0))).alias(c)\n",
    "            if c in numeric_cols else\n",
    "            # Categorical → fill with literal \"missing\"\n",
    "            F.coalesce(F.col(c), F.lit(\"missing\")).alias(c)\n",
    "            if c in categorical_cols else\n",
    "            # Everything else unchanged (date, target, etc.)\n",
    "            F.col(c)\n",
    "            for c in df.columns\n",
    "        ]\n",
    "    )\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# Force evaluation\n",
    "df.count()\n",
    "\n",
    "print(f\"\\nIMPUTATION COMPLETED IN {time.time() - start_time:.1f} seconds\")\n",
    "print(f\"Final clean & imputed dataset: {df.count():,} rows\")\n",
    "print(f\"Features: {len(feature_columns)} → {feature_columns}\")\n",
    "print(\"Target : sqm_price (we will take log1p later).\")\n",
    "\n",
    "# Quick sanity check – any nulls left in features?\n",
    "remaining_nulls = df.select(\n",
    "    [F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in feature_columns]\n",
    ").first()\n",
    "\n",
    "print(\n",
    "    \"Remaining nulls in features:\",\n",
    "    {c: remaining_nulls[c] for c in feature_columns if remaining_nulls[c] > 0}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b13581",
   "metadata": {},
   "source": [
    "# Define Target Encoding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Target Encoding Implementation for Danish Housing Price Prediction\n",
    "\n",
    "This script replaces the StringIndexer approach with Target Encoding for high-cardinality\n",
    "features (city and zip_code) while keeping StringIndexer for low-cardinality features.\n",
    "\n",
    "Add this code to your notebook AFTER the missing value imputation section and \n",
    "BEFORE the train/val/test split.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IMPLEMENTING TARGET ENCODING FOR HIGH-CARDINALITY FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Define Target Encoding Functions\n",
    "# =============================================================================\n",
    "\n",
    "def fit_target_encoder(train_df, cat_col, target_col, smoothing=10):\n",
    "    \"\"\"\n",
    "    Fit target encoder on training data and return encoding map.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        cat_col: Categorical column name\n",
    "        target_col: Target column name (e.g., 'sqm_price')\n",
    "        smoothing: Smoothing parameter to prevent overfitting (default=10)\n",
    "    \n",
    "    Returns:\n",
    "        encoding_map: DataFrame with category and encoded value\n",
    "        global_mean: Global mean of target for handling unseen categories\n",
    "    \"\"\"\n",
    "    # Calculate global mean\n",
    "    global_mean = train_df.select(F.mean(target_col)).first()[0]\n",
    "    \n",
    "    # Calculate category statistics\n",
    "    cat_stats = train_df.groupBy(cat_col).agg(\n",
    "        F.mean(target_col).alias('cat_mean'),\n",
    "        F.count(target_col).alias('cat_count')\n",
    "    )\n",
    "    \n",
    "    # Apply smoothing formula:\n",
    "    # encoded_value = (cat_mean * cat_count + global_mean * smoothing) / (cat_count + smoothing)\n",
    "    encoding_map = cat_stats.withColumn(\n",
    "        f'{cat_col}_encoded',\n",
    "        (F.col('cat_mean') * F.col('cat_count') + F.lit(global_mean) * F.lit(smoothing)) / \n",
    "        (F.col('cat_count') + F.lit(smoothing))\n",
    "    ).select(cat_col, f'{cat_col}_encoded')\n",
    "    \n",
    "    return encoding_map, global_mean\n",
    "\n",
    "\n",
    "def transform_target_encoder(df, cat_col, encoding_map, global_mean):\n",
    "    \"\"\"\n",
    "    Apply fitted target encoder to new data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to transform\n",
    "        cat_col: Categorical column name\n",
    "        encoding_map: Encoding map from fit_target_encoder\n",
    "        global_mean: Global mean for handling unseen categories\n",
    "    \n",
    "    Returns:\n",
    "        df_encoded: DataFrame with new encoded column\n",
    "    \"\"\"\n",
    "    # Join encoding map\n",
    "    df_encoded = df.join(encoding_map, on=cat_col, how='left')\n",
    "    \n",
    "    # Fill missing values (unseen categories) with global mean\n",
    "    df_encoded = df_encoded.fillna({f'{cat_col}_encoded': global_mean})\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Apply Target Encoding to High-Cardinality Features\n",
    "# =============================================================================\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Define which features to target encode (high cardinality)\n",
    "target_encode_cols = ['city', 'zip_code']\n",
    "\n",
    "# Define which features to string index (low cardinality)\n",
    "string_index_cols = ['house_type', 'sales_type', 'area', 'region']\n",
    "\n",
    "print(f\"\\nTarget encoding columns: {target_encode_cols}\")\n",
    "print(f\"String indexing columns: {string_index_cols}\")\n",
    "\n",
    "# First, we need to split the data BEFORE encoding to prevent data leakage\n",
    "# Create label column first\n",
    "df_with_label = df.withColumn(\"label_log_sqm_price\", F.log1p(F.col(\"sqm_price\")).cast(\"double\"))\n",
    "\n",
    "# Temporal split (same as original)\n",
    "train_df = df_with_label.filter(F.year(\"date\") <= 2020).cache()\n",
    "val_df = df_with_label.filter((F.year(\"date\") >= 2021) & (F.year(\"date\") <= 2022)).cache()\n",
    "test_df = df_with_label.filter(F.year(\"date\") >= 2023).cache()\n",
    "\n",
    "print(\"\\nData split completed:\")\n",
    "print(f\"  Train: {train_df.count():,} rows\")\n",
    "print(f\"  Val  : {val_df.count():,} rows\")\n",
    "print(f\"  Test : {test_df.count():,} rows\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: Fit Target Encoders on Training Data Only\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nFitting target encoders on training data...\")\n",
    "encoders = {}\n",
    "global_means = {}\n",
    "\n",
    "for col in target_encode_cols:\n",
    "    print(f\"  - Fitting encoder for '{col}'...\")\n",
    "    encoding_map, global_mean = fit_target_encoder(\n",
    "        train_df, \n",
    "        cat_col=col, \n",
    "        target_col='sqm_price',  # Use original target, not log-transformed\n",
    "        smoothing=10  # Adjust this parameter if needed\n",
    "    )\n",
    "    encoders[col] = encoding_map\n",
    "    global_means[col] = global_mean\n",
    "    \n",
    "    # Show some statistics\n",
    "    n_categories = encoding_map.count()\n",
    "    print(f\"    → {n_categories} unique categories, global mean: {global_mean:.2f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: Transform All Datasets\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nTransforming datasets with target encoding...\")\n",
    "\n",
    "# Transform train set\n",
    "for col in target_encode_cols:\n",
    "    train_df = transform_target_encoder(train_df, col, encoders[col], global_means[col])\n",
    "    print(f\"  ✓ Train: '{col}' → '{col}_encoded'\")\n",
    "\n",
    "# Transform validation set\n",
    "for col in target_encode_cols:\n",
    "    val_df = transform_target_encoder(val_df, col, encoders[col], global_means[col])\n",
    "    print(f\"  ✓ Val  : '{col}' → '{col}_encoded'\")\n",
    "\n",
    "# Transform test set\n",
    "for col in target_encode_cols:\n",
    "    test_df = transform_target_encoder(test_df, col, encoders[col], global_means[col])\n",
    "    print(f\"  ✓ Test : '{col}' → '{col}_encoded'\")\n",
    "\n",
    "print(\"\\nApplying StringIndexer to low-cardinality features...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1a1fa",
   "metadata": {},
   "source": [
    "# Apply String Indexing to Low-Cardinality Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b127adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(\n",
    "        inputCol=col,\n",
    "        outputCol=col + \"_indexed\",\n",
    "        handleInvalid=\"keep\",\n",
    "        stringOrderType=\"frequencyDesc\"\n",
    "    )\n",
    "    for col in string_index_cols\n",
    "]\n",
    "\n",
    "indexing_pipeline = Pipeline(stages=indexers)\n",
    "\n",
    "# Fit on training data\n",
    "print(\"  - Fitting indexers on training data...\")\n",
    "indexer_model = indexing_pipeline.fit(train_df)\n",
    "\n",
    "# Transform all datasets\n",
    "print(\"  - Transforming all datasets...\")\n",
    "train_df = indexer_model.transform(train_df).cache()\n",
    "val_df = indexer_model.transform(val_df).cache()\n",
    "test_df = indexer_model.transform(test_df).cache()\n",
    "\n",
    "# Force materialization\n",
    "train_df.count()\n",
    "val_df.count()\n",
    "test_df.count()\n",
    "\n",
    "print(f\"\\n✓ String indexing completed for: {string_index_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0beaae",
   "metadata": {},
   "source": [
    "# Prepare Feature List for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e12e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated feature list (using encoded columns instead of indexed for city/zip_code)\n",
    "features_to_use = [\n",
    "    # Low-cardinality features (string indexed)\n",
    "    \"house_type_indexed\",\n",
    "    \"sales_type_indexed\",\n",
    "    \"area_indexed\",\n",
    "    \"region_indexed\",\n",
    "    # High-cardinality features (target encoded)\n",
    "    \"city_encoded\",\n",
    "    \"zip_code_encoded\",\n",
    "    # Numeric features\n",
    "    \"sqm\",\n",
    "    \"%_change_between_offer_and_purchase\",\n",
    "    \"nom_interest_rate%\",\n",
    "    \"dk_ann_infl_rate%\",\n",
    "    \"yield_on_mortgage_credit_bonds%\",\n",
    "    \"year_build\",\n",
    "    \"no_rooms\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "]\n",
    "\n",
    "print(f\"\\nFinal feature list ({len(features_to_use)} features):\")\n",
    "for i, feat in enumerate(features_to_use, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TARGET ENCODING COMPLETED IN {elapsed_time:.1f} seconds\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "print(\"Training Gradient Boosted Trees (300 trees, depth=12)...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=60\n",
    "    ,          # number of trees\n",
    "    maxDepth=8,\n",
    "    maxBins=1024,         # safe for many categories (e.g. zip_code)\n",
    "    subsamplingRate=0.8,\n",
    "    featureSubsetStrategy=\"sqrt\",\n",
    "    seed=42,\n",
    "    lossType=\"squared\"\n",
    ")\n",
    "\n",
    "gbt_model = gbt.fit(train_ready)\n",
    "\n",
    "training_time = (time.time() - start) / 60\n",
    "print(f\"Training completed in {training_time:.1f} minutes\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad6f37",
   "metadata": {},
   "source": [
    "# Create Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeeb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features_to_use,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Assemble features and keep only (features, label)\n",
    "train_ready = assembler.transform(train_df) \\\n",
    "    .select(\"features\", \"label_log_sqm_price\") \\\n",
    "    .withColumnRenamed(\"label_log_sqm_price\", \"label\")\n",
    "\n",
    "val_ready = assembler.transform(val_df) \\\n",
    "    .select(\"features\", \"label_log_sqm_price\") \\\n",
    "    .withColumnRenamed(\"label_log_sqm_price\", \"label\")\n",
    "\n",
    "test_ready = assembler.transform(test_df) \\\n",
    "    .select(\"features\", \"label_log_sqm_price\") \\\n",
    "    .withColumnRenamed(\"label_log_sqm_price\", \"label\")\n",
    "\n",
    "# Cache final datasets\n",
    "train_ready.cache().count()\n",
    "val_ready.cache().count()\n",
    "test_ready.cache().count()\n",
    "\n",
    "print(\"\\n✓ Feature vectors created and cached\")\n",
    "print(\"✓ Ready for model training!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENCODING EXAMPLES (Sample from training data)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show some examples of encoded values\n",
    "sample_df = train_df.select('city', 'city_encoded', 'zip_code', 'zip_code_encoded', 'sqm_price') \\\n",
    "    .orderBy(F.rand()) \\\n",
    "    .limit(10)\n",
    "\n",
    "sample_df.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nNote: Encoded values represent the smoothed average sqm_price for each category\")\n",
    "print(\"This captures the relationship between location and price directly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f97795",
   "metadata": {},
   "source": [
    "# Evaluate on TEST set (2023–2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b942d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = gbt_model.transform(test_ready)\n",
    "\n",
    "# Back-transform predictions from log1p scale to DKK/m²\n",
    "pred_test = (\n",
    "    pred_test\n",
    "    .withColumn(\"pred_price_m2\",   F.expm1(F.col(\"prediction\")))\n",
    "    .withColumn(\"actual_price_m2\", F.expm1(F.col(\"label\")))\n",
    ")\n",
    "\n",
    "# Compute MAPE\n",
    "mape = (\n",
    "    pred_test\n",
    "    .withColumn(\n",
    "        \"abs_percent_error\",\n",
    "        F.abs(F.col(\"pred_price_m2\") - F.col(\"actual_price_m2\")) / F.col(\"actual_price_m2\")\n",
    "    )\n",
    "    .select(F.mean(\"abs_percent_error\"))\n",
    "    .first()[0] * 100\n",
    ")\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"DANISH HOUSING PRICE MODEL — FINAL RESULT (2023–2024 TEST SET)\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"Test MAPE (2023–2024)     : {mape:.2f}%\")\n",
    "print(f\"Training time             : {training_time:.1f} minutes\")\n",
    "print(f\"Features used             : {len(features_to_use)}\")\n",
    "print(f\"Model                     : GBTRegressor (300 trees, depth=12)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Top 12 most important features\n",
    "# --------------------------------------------------------------\n",
    "importances = gbt_model.featureImportances.toArray()\n",
    "feat_importance = sorted(zip(importances, features_to_use), reverse=True)[:12]\n",
    "\n",
    "print(\"\\nTOP 12 MOST IMPORTANT FEATURES:\")\n",
    "for imp, feat in feat_importance:\n",
    "    print(f\"  {imp:6.1%} → {feat}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Sample predictions: recent sales (NO RETRAIN – join back metadata)\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\nSample predictions (most recent sales):\")\n",
    "\n",
    "pred_with_info = (\n",
    "    pred_test\n",
    "    .join(\n",
    "        test_df.select(\n",
    "            F.col(\"label_log_sqm_price\").alias(\"label\"),\n",
    "            \"date\",\n",
    "            \"city\",\n",
    "            \"house_type\",\n",
    "            \"sqm\",\n",
    "        ),\n",
    "        on=\"label\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "pred_with_info.select(\n",
    "    \"date\",\n",
    "    \"city\",\n",
    "    \"house_type\",\n",
    "    \"sqm\",\n",
    "    F.round(\"actual_price_m2\", 0).alias(\"actual_DKK_m2\"),\n",
    "    F.round(\"pred_price_m2\",   0).alias(\"pred_DKK_m2\"),\n",
    "    F.round(\n",
    "        (F.col(\"pred_price_m2\") - F.col(\"actual_price_m2\"))\n",
    "        / F.col(\"actual_price_m2\") * 100,\n",
    "        1\n",
    "    ).alias(\"error_%\")\n",
    ").orderBy(F.desc(\"date\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "pred_test.cache().count()\n",
    "\n",
    "# RMSE, MAE, R2 on log scale (label = log1p(sqm_price))\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "rmse = rmse_evaluator.evaluate(pred_test)\n",
    "mae  = mae_evaluator.evaluate(pred_test)\n",
    "r2   = r2_evaluator.evaluate(pred_test)\n",
    "\n",
    "print(\"=== Test metrics (log(sqm_price)) ===\")\n",
    "print(f\"RMSE  : {rmse:.4f}\")\n",
    "print(f\"MAE   : {mae:.4f}\")\n",
    "print(f\"R^2   : {r2:.4f}\")\n",
    "print(f\"MAPE% : {mape:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566fbc74",
   "metadata": {},
   "source": [
    "# SCATTER: PREDICTED VS ACTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Take a sample from test predictions (to avoid pulling millions of rows)\n",
    "sample_pd = (\n",
    "    pred_test\n",
    "    .select(\"actual_price_m2\", \"pred_price_m2\")\n",
    "    .sample(False, 0.05, seed=42)   # ~5% of test\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(sample_pd[\"actual_price_m2\"], sample_pd[\"pred_price_m2\"], alpha=0.3)\n",
    "max_val = max(sample_pd[\"actual_price_m2\"].max(), sample_pd[\"pred_price_m2\"].max())\n",
    "plt.plot([0, max_val], [0, max_val])          # perfect prediction line\n",
    "\n",
    "plt.xlabel(\"Actual price (DKK / m²)\")\n",
    "plt.ylabel(\"Predicted price (DKK / m²)\")\n",
    "plt.title(\"Predicted vs Actual – test set\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833065f",
   "metadata": {},
   "source": [
    "# MAPE BY REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183775ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Join predictions with region + city info\n",
    "# df_with_label has label_log_sqm_price, region, city, date, sqm_price, etc.\n",
    "pred_with_region = (\n",
    "    pred_test\n",
    "    .join(\n",
    "        df_with_label.select(\n",
    "            F.col(\"label_log_sqm_price\").alias(\"label\"),\n",
    "            \"region\",\n",
    "            \"city\",\n",
    "            \"date\",\n",
    "            \"sqm_price\"\n",
    "        ),\n",
    "        on=\"label\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compute MAPE per region\n",
    "mape_by_region = (\n",
    "    pred_with_region\n",
    "    .withColumn(\n",
    "        \"abs_percent_error\",\n",
    "        F.abs(F.col(\"pred_price_m2\") - F.col(\"actual_price_m2\")) / F.col(\"actual_price_m2\")\n",
    "    )\n",
    "    .groupBy(\"region\")\n",
    "    .agg(F.mean(\"abs_percent_error\").alias(\"mape\"))\n",
    "    .orderBy(\"mape\")\n",
    ")\n",
    "\n",
    "mape_region_pd = mape_by_region.toPandas()\n",
    "\n",
    "print(mape_region_pd)\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(mape_region_pd[\"region\"], mape_region_pd[\"mape\"] * 100)\n",
    "plt.ylabel(\"MAPE (%)\")\n",
    "plt.title(\"MAPE by region – test set\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
